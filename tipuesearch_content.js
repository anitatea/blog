var tipuesearch = {"pages":[{"title":"About","text":"About the author of this blog Profession Well, as you can see I like wallace and gromit. I'm looking for opportunities as a Data Science and like Python most. I am able to write HTML/CSS/JavaScript but it's definitely not my favorite. Ehm, even if I don't like it very much, it's still useful here and there. Trying to get warm with golang (dislike absence of exceptions) and hater of ruby/perl. NodeJS is cool as well. Bash is very useful but ugly as hell. Spare time Then in my spare time, I craft jewelry. And pop-up cards. Then I love to sweat in hot yoga and try to do it as often as I can without having to go into public after looking like I hopped out of the pool. I'm always learning and I have a hell of fun doing it.","tags":"en","url":"https://anitatea.github.io/blog/pages/about/","loc":"https://anitatea.github.io/blog/pages/about/"},{"title":"Using emergency room hospital data to predict wait times","text":"Have you ever stepped foot in the emergency room of a hospital in Toronto either for a loved one or perhaps yourself, only to be met with a grueling long wait until you see any medical professional? It sucks. And there should definitely be a better system, right? Having volunteered at Toronto Western Hospital Emergency Department and being able to experience both sides of the system, I wanted to dig deep at what's really going on and to actually do something about it. Technologies Used Python Pandas pipenv - for local storage of credentials Beautiful Soup - Web Scraping SKLearn pipelines DataFrameMapper HDBSCAN clustering Flask *Google Maps API Introduction PATIENTLY WAITING was created as my capstone project for the Data Science Immersive program at Bitmaker / General Assembly. A beta version of this application can be found here! Check emergency room wait times around Toronto by choosing a day: https://patientlywaiting.herokuapp.com/ Data Gathering Detailed administrative data on date, patient flow, current and past examinations in Ontario was provided by: * National Ambulatory Care Reporting System (NACRS) Canadian Institude for Health Information (CIHI) Data Modeling The ability to accurately and reliably predict waiting times at walk-in hospital facilities can increase both patient satisfaction and hospital efficiency via a better management of patient flow. My web-app implements machine learning (ML) models to predict waiting times in the Emergency Room (ER) of the largest public hospital in the Greater Toronto Area (GTA). Several machine learning (ML) techniques were evaluated to find the most accurate and useful prediction to a user. I chose Gradient Boosting among other regression models explored for predicting wait times. PATIENTLY WAITING is currently in beta testing. If you notice any bugs, have any questions or suggestions, I'd love to hear from you: anita.tran38@gmail.com . Planned Future Enhancements Docker for hosting database, nginx and flask web app Google API to read your location Actively scrape hospital data as it is released per month on hqontatio.ca Generate best route to hospital using combinatorial optimization If you'd like to play around with the tool, a beta version is running here: https://patientlywaiting.herokuapp.com/ The complete project source code (which is still in the process of being updated) can be found on GitHub .","tags":"Projects","url":"https://anitatea.github.io/blog/2020/03/patientlywaiting.html","loc":"https://anitatea.github.io/blog/2020/03/patientlywaiting.html"},{"title":"What's a customer worth? Modeling customer lifetime value","text":"Recently, I've been taking advantage of an app called Sweatabl to discover free workout classes near me. While I love to switch up my workout routine and explore the fitness community freely, I can't help but wonder that this definitely makes for a much trickier Customer Lifetime Value (CLV) calculation. Whether in E-Commerce, retail or workout classes, there is an investment in customers (acquisition costs, offline ads, promotions, discounts, etc.) to generate revenue and be profitable. These actions make some customers incredibly valuable in terms of lifetime value for the business. Other customers do go away, however they do so silently; they don't have to tell us they're leaving. I wanted to model customer data and see how it can be used to predict a customer's lifetime value. The data we'll use is an online retail dataset downloaded from UCI Machine Learning Repository . Import the data import pandas as pd import warnings warnings . filterwarnings ( 'ignore' ) df = pd . read_excel ( \"Online Retail.xlsx\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country 0 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 2010-12-01 08:26:00 2.55 17850.0 United Kingdom 1 536365 71053 WHITE METAL LANTERN 6 2010-12-01 08:26:00 3.39 17850.0 United Kingdom 2 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 2010-12-01 08:26:00 2.75 17850.0 United Kingdom 3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 2010-12-01 08:26:00 3.39 17850.0 United Kingdom 4 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 2010-12-01 08:26:00 3.39 17850.0 United Kingdom Clean the data and make a new dataframe. For our analysis, all we need is CustomerID, InvoiceDate (without the time) and we'll make a new columns called 'Sales'. import datetime as dt df [ 'InvoiceDate' ] = pd . to_datetime ( df [ 'InvoiceDate' ]) . dt . date df = df [ pd . notnull ( df [ 'CustomerID' ])] df = df [( df [ 'Quantity' ] > 0 )] df [ 'Sales' ] = df [ 'Quantity' ] * df [ 'UnitPrice' ] df = df [[ 'CustomerID' , 'InvoiceDate' , 'Sales' ]] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CustomerID InvoiceDate Sales 0 17850.0 2010-12-01 15.30 1 17850.0 2010-12-01 20.34 2 17850.0 2010-12-01 22.00 3 17850.0 2010-12-01 20.34 4 17850.0 2010-12-01 20.34 Let's see how many unique customers. df [ 'CustomerID' ] . nunique () 4339 Shape of your data For CLV models, the following nomenclature is used: Frequency : number of repeat purchases the customer has made. This means that it's one less than the total number of purchases. T : age of the customer in whatever time units chosen (daily, in our dataset). This is equal to the duration between a customer's first purchase and the end of the period under study. Recency : age of the customer when they made their most recent purchases. This is equal to the duration between a customer's first purchase and their latest purchase. (Thus if they have made only 1 purchase, the recency is 0.) We'll be using the Lifetimes package , developed by Cameron Davidson-Pilon, data scientist at Shopify. If your data is not in this format, there are useful functions in the documentation to transform your data. from lifetimes.plotting import * from lifetimes.utils import * data = summary_data_from_transaction_data ( df , 'CustomerID' , 'InvoiceDate' , monetary_value_col = 'Sales' , observation_period_end = '2011-12-9' ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frequency recency T monetary_value CustomerID 12346.0 0.0 0.0 325.0 0.000000 12347.0 6.0 365.0 367.0 599.701667 12348.0 3.0 283.0 358.0 301.480000 12349.0 0.0 0.0 18.0 0.000000 12350.0 0.0 0.0 310.0 0.000000 CustomerID 12346 made one purchase only (no repeat), so his frequency and recency are 0 and their age is 325 days (e.g. the duration between his first purchase and the end of the period in the analysis). Frequency/Recency analysis using the BG/NBD model from lifetimes import BetaGeoFitter bgf = BetaGeoFitter ( penalizer_coef = 0.0 ) bgf . fit ( data [ 'frequency' ], data [ 'recency' ], data [ 'T' ]) print ( bgf ) <lifetimes.BetaGeoFitter: fitted with 4339 subjects, a: 0.00, alpha: 68.89, b: 6.75, r: 0.83> Let's say a customer made a purchase every day for three weeks straight and then hasn't had any activity for months. Chances are the probability of this customer being \"alive\" is very slim. On the other hand, if a customer consistently makes a purchase every couple of months, they're more likely to be \"alive\". from lifetimes.plotting import plot_frequency_recency_matrix import matplotlib.pyplot as plt % matplotlib inline fig = plt . figure ( figsize = ( 12 , 8 )) plot_frequency_recency_matrix ( bgf ); Customers who have purchased frequently and purchased recently will likely be the best customers in the future. (bottom-right corner). Customers who have purchased a lot but not recently (top-right corner), have probably gone. Let's go back to our customers and rank their expected number of purchases. t = 1 data [ 'predicted_purchases' ] = bgf . conditional_expected_number_of_purchases_up_to_time ( t , data [ 'frequency' ], data [ 'recency' ], data [ 'T' ]) data . sort_values ( by = 'predicted_purchases' ) . tail ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frequency recency T monetary_value predicted_purchases CustomerID 14606.0 88.0 372.0 373.0 135.890114 0.201005 15311.0 89.0 373.0 373.0 677.729438 0.203269 17841.0 111.0 372.0 373.0 364.452162 0.253053 12748.0 113.0 373.0 373.0 298.360885 0.257581 14911.0 131.0 372.0 373.0 1093.661679 0.298312 Assessing model fit Let's see how correct our model actually is. We can compare our data versus artificial data simulated with our fitted model's parameters. from lifetimes.plotting import plot_period_transactions plot_period_transactions ( bgf ); # train test split is being done underneath We now partition the dataset into a calibration period dataset and a holdout dataset - just like cross-validation in machine learning models. This is important as we want to test how our model performs on data not yet seen. from lifetimes.utils import calibration_and_holdout_data summary_cal_holdout = calibration_and_holdout_data ( df , 'CustomerID' , 'InvoiceDate' , calibration_period_end = '2011-06-08' , observation_period_end = '2011-12-9' ) print ( summary_cal_holdout . head ()) frequency_cal recency_cal T_cal frequency_holdout \\ CustomerID 12346 . 0 0 . 0 0 . 0 141 . 0 0 . 0 12347 . 0 2 . 0 121 . 0 183 . 0 4 . 0 12348 . 0 2 . 0 110 . 0 174 . 0 1 . 0 12350 . 0 0 . 0 0 . 0 126 . 0 0 . 0 12352 . 0 3 . 0 34 . 0 112 . 0 3 . 0 duration_holdout CustomerID 12346 . 0 184 12347 . 0 184 12348 . 0 184 12350 . 0 184 12352 . 0 184 Perform fitting on the _cal columns, and test on the _holdout columns. from lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases bgf . fit ( summary_cal_holdout [ 'frequency_cal' ], summary_cal_holdout [ 'recency_cal' ], summary_cal_holdout [ 'T_cal' ]) plot_calibration_purchases_vs_holdout_purchases ( bgf , summary_cal_holdout ); Predicting customer transactions Let's say we want to predict the purchases of CustomerID = 12347 in the next 10 days. t = 10 # predict purchases in 10 periods individual = data . loc [ 12347 ] # below function is an alias to `bfg.conditional_expected_number_of_purchases_up_to_time` bgf . predict ( t , individual [ 'frequency' ], individual [ 'recency' ], individual [ 'T' ]) 0.1572774363164109 Our model is saying that this customer's future transaction is 0.157 in 10 days. Note that this is not a percentage, but what an individual's future purchases might look like. Estimating customer lifetime value using Gamma-Gamma model of monetary value In our analysis, we haven't yet taken into account the economic value (Sales) of each transaction and only focused on the occurrences. We want to predict the likely spend per transaction in the future for a customer. To estimate the economic value, we use the Gamma-Gamma submodel. For this estimation, we filter for customers who had at least one repeat purchase. returning_customers = data [ data [ 'frequency' ] > 0 ] returning_customers . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frequency recency T monetary_value predicted_purchases CustomerID 12347.0 6.0 365.0 367.0 599.701667 0.015656 12348.0 3.0 283.0 358.0 301.480000 0.008956 12352.0 6.0 260.0 296.0 368.256667 0.018697 12356.0 2.0 303.0 325.0 269.905000 0.007172 12358.0 1.0 149.0 150.0 683.200000 0.008340 len ( returning_customers ) 2790 Apply the Gamma-Gamma model. from lifetimes import GammaGammaFitter ggf = GammaGammaFitter ( penalizer_coef = 0 ) ggf . fit ( returning_customers [ 'frequency' ], returning_customers [ 'monetary_value' ]) print ( ggf ) <lifetimes.GammaGammaFitter: fitted with 2790 subjects, p: 2.10, q: 3.45, v: 485.57> Estimate of average transaction value for each customer. print ( ggf . conditional_expected_average_profit ( data [ 'frequency' ], data [ 'monetary_value' ] ) . head ( 10 )) CustomerID 12346.0 416.917667 12347.0 569.988807 12348.0 333.762672 12349.0 416.917667 12350.0 416.917667 12352.0 376.166864 12353.0 416.917667 12354.0 416.917667 12355.0 416.917667 12356.0 324.008941 dtype: float64 Finally, computing the total CLV! We'll be using the Discounted Cash Flow method adjusting for cost of capital. # refit the BG model to the data (with money value) dataset bgf . fit ( data [ 'frequency' ], data [ 'recency' ], data [ 'T' ]) CLV = ggf . customer_lifetime_value ( bgf , #the model to use to predict the number of future transactions data [ 'frequency' ], data [ 'recency' ], data [ 'T' ], data [ 'monetary_value' ], time = 12 , # months discount_rate = 0.01 # monthly discount rate ~ 12.7% annually ) CLV . sort_values ( ascending = False ) . head ( 10 ) CustomerID 14646.0 222128.930290 18102.0 178895.333435 16446.0 175531.468535 17450.0 147476.621010 14096.0 127589.202889 14911.0 109442.132668 12415.0 96290.227222 14156.0 89410.334970 17511.0 67660.407580 16029.0 58729.618772 Name: clv, dtype: float64 CustomerID 14646 is estimated to have a CLV of over $200K. Using this analysis, I can see the value it can bring such as identifying traits and features of valuable customers.","tags":"blog","url":"https://anitatea.github.io/blog/2020/02/CLV.html","loc":"https://anitatea.github.io/blog/2020/02/CLV.html"},{"title":"Are you done with fake (news, friends, etc.)?","text":"The Fitbit I got during the holidays got me thinking. How do these health app developers get their data to test their data? Do they really survey thousands of people about their personal data or manually input health data one by one? Isn't there a more efficient way to get this data? img src = <\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fmemecrunch.com%2Fmeme%2F7VYF%2Ffake-people%2Fimage.png&f=1&nofb=1> For this blog, let me tell you where \"fake\" things (data) will come in handy. Introducing Mimisis The ability to generate mock but valid data comes in handy in app development, where you need to work with databases. Filling in the database by hand is a time-consuming and tedious process, which can be done in three stages — gathering necessary information, post-processing the data and coding the data generator itself. It gets really complicated when you need to generate not just 10–15 users, but 100–150 thousand users (or other types of data). In this article as well as the two following ones we will introduce you to a tool, which immensely simplifies generating mock data, initial database loading and testing in general. Mimesis is a Python library, which helps generate mock data for various purposes. The library was written with the use of tools from the standard Python library, and therefore, it doesn't have any side dependencies. Currently the library supports 32 languages and 21 class providers, supplying various data. from mimesis import Person , Generic , Address person = Person ( 'en' ) person . full_name () 'Tory Parker' Our regular imports: from mimesis import Person , Science , Text import pandas as pd Instantiate our languages: person = Person ( 'en' ) science = Science ( 'en' ) text = Text ( 'en' ) For this example, I'll make 50 patients: data = [] for _ in range ( 0 , 50 ): data . append (( person . full_name (), person . gender (), person . age (), person . weight (), person . height (), science . dna_sequence ( length = 10 ), science . rna_sequence ( length = 10 ), text . answer ())) data = pd . DataFrame ( data ) data . columns = [ 'name' , 'gender' , 'age' , 'weight(kg)' , 'height(m)' , 'dna_sequence' , 'rna_sequence' , 'smoker' ]","tags":"blog","url":"https://anitatea.github.io/blog/2020/02/mimesis.html","loc":"https://anitatea.github.io/blog/2020/02/mimesis.html"},{"title":"Epic visualizations to plot epidemics","text":"Data visualization is one of my favourite creative outlets - to draw out deeper insights from meaningful data. Like this. (Image source: Graphjam.com ) Visualization packages in Python is far from scarce and there are great options for static graphs. However, as data gets more complex and understanding becomes more distant, there is an obvious need for interactive visualizations to more easily explore data. What's Dash \"Bringing data science out of the lab and into the business. Formally only available for enterprise, Dash is now an open source library (free!) for creating interactive web-based visualizations. Highly interactive web application using Python or R. HTML/CSS/JavaScript not required (... and the door is that-a-way) . What I think makes it powerful is it is able to incorporate all the functionalities of Python for data manipulation and much more. Amazon, Cisco, Shell, among other worldwide enterprises have been embracing this tool for large scale deployments. Let's Get Started Pretty straight forward. More details for updates on the Dash installation page . pip install dash Sample Interactive Bar Chart I used data from the Centers for Disease Control and Prevention on Drug Poisoning Mortality by State in the United States . I'm going to build an interactive bar chart that shows absolute deaths per state in 2016 and 2017. (The dataset has been updated on April 29, 2019 and so I believe this is the most accurate dataset to this date.) Bring in all the dash modules and pandas for reading and manipulating the data. I saved this file as bar_app.py : import dash import dash_core_components as dcc import dash_html_components as html import plotly.graph_objs as go import pandas as pd # to manipulate our data For this simple example, I'm choosing to import the plotly.graph_objs whereas the documentation uses dcc.graph to build the plot. Writing this article, I felt that it was easier to use the plotly graph object since there were a lot more examples online than there were the plain dcc.graph for the simple cases. This is my preference and I do find it more straightforward to demonstrate this example. Reading in the first few pieces of data and filtering for deaths by state in 2016 and 2017: df = pd . read_csv ( 'data/NCHS_-_Drug_Poisoning_Mortality_by_State__United_States.csv' ) df . iloc [:,: 7 ] . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } State Year Sex Age Group Race and Hispanic Origin Deaths Population 0 Alabama 1999 Both Sexes All Ages All Races-All Origins 169 4430143 1 Alabama 2000 Both Sexes All Ages All Races-All Origins 197 4447100 2 Alabama 2001 Both Sexes All Ages All Races-All Origins 216 4467634 3 Alabama 2002 Both Sexes All Ages All Races-All Origins 211 4480089 4 Alabama 2003 Both Sexes All Ages All Races-All Origins 197 4503491 # I only need State, Year and Deaths column df = df [[ 'State' , 'Year' , 'Deaths' ]] # check unique states df [ 'State' ] . unique () array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'United States', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object) # 'United States' is obviously not a state. I suspect it captures deaths that are uncategorized. # For our example, we'll drop these rows. indexUS = df [ df [ 'State' ] == 'United States' ] . index df = df . drop ( indexUS ) # filter for 2016 df_2016 = df [ df [ 'Year' ] == 2016 ] # filter for 2016 df_2017 = df [ df [ 'Year' ] == 2017 ] Now that the data is simply tabulated by state and total deaths; we can build out the bars in our graph. The convention for plotly is that each item being plotted is usually called a trace . In this example, we want to plot State vs. Deaths. trace1 = go . Bar ( x = df_2016 [ 'State' ], y = df_2016 [ 'Deaths' ], name = '2016' , marker = dict ( color = 'rgb(55, 83, 109)' ) # darker blue ) trace2 = go . Bar ( x = df_2017 [ 'State' ], y = df_2017 [ 'Deaths' ], name = '2017' , marker = dict ( color = 'rgb(26, 118, 255)' ) # lighter blue ) Now were ready to build out the actual Dash app. app = dash . Dash () app . layout = html . Div ( children = [ html . H1 ( children = 'Drug Poisoning in the United States in 2016 & 2017' ), html . Div ( children = '''(Hover over bars to view death count)''' ), dcc . Graph ( id = 'example-graph' , figure = { 'data' : [ trace1 , trace2 ], 'layout' : go . Layout ( title = 'Drug Poisoning Mortality by State in 2016 & 2017' , barmode = 'group' ) }) ]) Most of the structure is dash building the app, using HTML components and creating the figure dictionary, containing instructions to build the graph and customize layout. What I love is the simplicity of how dash integrates HTML components beautifully without having to deal with the standard HTML/CSS/JS boilerplate required for most modern web apps that you see. Since I'm dealing with various code styles, Atom is my preferred editor to manage matching brackets, parentheses and such. dcc.Graph defines the figure using a using a dictionary that contains the actual figure, plotted data and layout options. In this case, the layout was needed to define the title and bar type graph. Once the app is laid out, we need to make sure it can run: if __name__ == '__main__' : app . run_server ( debug = True ) Running this in your terminal: python bar_app . py Here's our nice interactive graph: Pretty great when you think how much engagement you can get with a simple application of 35 lines of code. You can find the above example available on github. Not only is the layout and look of dash clean and report-ready; Dash also took care of scaling the interactivity on different browsers. So we can focus on beautiful, pure python! The entire code: import dash import dash_core_components as dcc import dash_html_components as html import plotly.graph_objs as go import pandas as pd df = pd . read_csv ( 'NCHS_-_Drug_Poisoning_Mortality_by_State__United_States.csv' ) df = df [[ 'State' , 'Year' , 'Deaths' ]] indexUS = df [ df [ 'State' ] == 'United States' ] . index df = df . drop ( indexUS ) df_2016 = df [ df [ 'Year' ] == 2016 ] df_2017 = df [ df [ 'Year' ] == 2017 ] trace1 = go . Bar ( x = df_2016 [ 'State' ], y = df_2016 [ 'Deaths' ], name = '2016' , marker = dict ( color = 'rgb(55, 83, 109)' )) trace2 = go . Bar ( x = df_2017 [ 'State' ], y = df_2017 [ 'Deaths' ], name = '2017' , marker = dict ( color = 'rgb(26, 118, 255)' )) app = dash . Dash () app . layout = html . Div ( children = [ html . H1 ( children = 'Drug Poisoning in the United States in 2016 & 2017' ), html . Div ( children = '''(Hover over bars to view death count)''' ), dcc . Graph ( id = 'example-graph' , figure = { 'data' : [ trace1 , trace2 ], 'layout' : go . Layout ( title = 'Drug Poisoning Mortality by State in 2016 & 2017' , barmode = 'group' ) }) ]) if __name__ == '__main__' : app . run_server ( debug = True ) For Fun: A More Complex Example Dash's real power shines in its ability to combine complex interactions - again all with Python. Dash provides several interactive components out of the box including Dropdowns, Multi-Select Dropdowns, Mapping, Radio Buttons, Checkboxes, Sliders, and Text Input. All of them can be easily constructed and tied into your plots to drive various interactive options. For a more complex demo, check out my dashboard using the same data, featuring: * Slider between years of 2012 - 2017 * Fully interactive heatmap of the United States * Drop down selection to show total counts of death Here's the link to the dashboard. Layout inspired by examples from the Dash-Plotly gallery. Final Thoughts I've only scratched the surface of this powerful yet simplistic tool. My favourite pieces: * Quick for building web-based visualization tools without prior knowledge of JavaScript and other web technologies * Default dashboard is already responsive * Ability to save, zoom, pan and interact with the display, on the display Presenting data-driven visualizations to non-technical colleagues can be a struggle, as if you are speaking two different languages without Google Translate. I've seen more than my fair share of presentations and the truly successful ones were the most engaging with their audience.","tags":"blog","url":"https://anitatea.github.io/blog/2020/01/dash-plotly.html","loc":"https://anitatea.github.io/blog/2020/01/dash-plotly.html"},{"title":"Can you doodle with Python?","text":"Throughout all my years in elementary school to engineering at UofT, my notes have always been decorated with swirls, spirals and circular doodles. Well now all my notes are now digital... but my noodle still wants to doodle. Let's see how we can do this with Python. Math can help us do the trick. The Recamán Sequence Let me introduce: The Recamán Sequence. It features a simple sequence of integers, which processed with Python, can produce some beautiful doodles. Here are the rules: 1. Start at zero. 2. Every step you take will be 1 more than the last step. 3. If it's possible to step backward (negatively), do so. Backward steps are only allowed if the resulting location is positive (greater than zero) and if that number has not occurred in our sequence yet. Otherwise step forward. Here's an example. Let's look at the beginning of the sequence: Start at zero. - 0 Our step size will be 1. Stepping backward would put us at -1 (from 0) which is not allowed. We step forward. - 0 -> 1 Next step size is 2. Stepping backward would put us at -1 (from 1) therefore we will step forward again. - 0 -> 1 -> 3 Turtle Graphics A relatively simple library, using a relative cursor (the 'turtle') which we will be following on a Cartesian plane. It has three attributes: location, orientation and a pen (the 'turtle'). Anyone with Python can get started with Turtle! It's already pre-packaged in the Python standard library. The Code First, let's import the library and set-up our drawing canvas. import turtle window = turtle . Screen () window . setup ( width = 800 , height = 600 , startx = 10 , starty = 0.5 ) doodle = turtle . Turtle () # A good name for our turtle doodle . shape ( \"turtle\" ) doodle . speed ( 10 ) scale = 5 # Not a turtle setting - used to scale the drawing Now I'll move our turtle pen to start on the left. This will give our turtle more room to work with. doodle . penup () doodle . setpos ( - 390 , 0 ) # Set on a Cartesian plane, e.g. (x,y) doodle . pendown () A few other housekeeping items. doodle . color ( 'navy' ) # Our pen color doodle . fillcolor ( 'black' ) # Our turtle color current = 0 # Here's how we know where we are seen = [] # A list to keep track of where we've been Take a step forward if our result is positive and if we have not been there before. # Step increases by 1 each time for step_size in range ( 1 , 100 ): backwards = current - step_size # Step backwards if its positive and we've never been there before if backwards > 0 and backwards not in seen : doodle . setheading ( 90 ) # 90 degrees is pointing straight up # 180 degrees means \"draw a semicircle\" doodle . circle ( scale * step_size / 2 , 180 ) current = backwards seen . append ( current ) # Otherwise, go forward else : doodle . setheading ( 270 ) # 270 degrees is straight down doodle . circle ( scale * step_size / 2 , 180 ) current += step_size seen . append ( current ) turtle . done () All done! The concatenated code for this article is below for your reference to try the whole thing by yourself. import turtle window = turtle . Screen () window . setup ( width = 800 , height = 600 , startx = 10 , starty = 0.5 ) doodle = turtle . Turtle () # A good name for our turtle doodle . shape ( \"turtle\" ) doodle . speed ( 0 ) scale = 5 # Not a turtle setting - used to scale the drawing doodle . penup () doodle . setpos ( - 390 , 0 ) doodle . pendown () doodle . color ( 'navy' ) doodle . fillcolor ( 'black' ) current = 0 # Here's how we know where we are seen = [] # A list to keep track of where we've been # Step increases by 1 each time for step_size in range ( 1 , 100 ): backwards = current - step_size # Step backwards if its positive and we've never been there before if backwards > 0 and backwards not in seen : doodle . setheading ( 90 ) # 90 degrees is pointing straight up # 180 degrees means \"draw a semicircle\" doodle . circle ( scale * step_size / 2 , 180 ) current = backwards seen . append ( current ) # Otherwise, go forwards else : doodle . setheading ( 270 ) # 270 degrees is straight down doodle . circle ( scale * step_size / 2 , 180 ) current += step_size seen . append ( current ) turtle . done ()","tags":"blog","url":"https://anitatea.github.io/blog/2019/12/turtle.html","loc":"https://anitatea.github.io/blog/2019/12/turtle.html"},{"title":"Featuring... Feature Engineering!","text":"As someone with an engineering background, I was naturally drawn to the concept of Feature Engineering and excited to take my first dive into machine learning through finding organization and insights from messy data – real world data. Feature engineering is the way of extracting features (variables) from data and transforming them into features that are suitable for Machine Learning algorithms. There are 3 main steps: Feature Selection – All features aren't equal. Therefore certain features which are more important than others will affect the accuracy of the prediction model. Methods of Feature Selection include correlation coefficient scores, Ridge regression, LASSO, and ElasticNet. Feature Transformation – Filling missing data values, scaling, and binning are common forms of transformation. To reduce skewness of the data when we do this, a log scale is used. We all know real world data is messy Feature Extraction – Large datasets are generally found to be redundant, therefore we'd want to reduce dimensionality of these features. To do so, constructing combination of our features extracts the features important to our model. Suppose we're given flight time data and asked to predict the price of a flight based on the departure and arrival times. df = df [[ 'dep_time' , 'arrival_time' , 'duration' , 'prices' ]] df We want to have 'dep_time' and 'arrival_time' in similar formats that are easier to analyse. How could we simplify these features for our model (and us) to understand? One way to interpret this data is we can bin the times to sections of time in a 24 hour scale. Night [0-5], Morning [6-11], Afternoon [12-17] and Evening [18-23] will be our bins. Note: For 'Arrival Time', we can drop the date landed, as it is captured in 'Duration'. Binning Departure Time ('dep_time'): df [ 'departure_t' ] = pd . to_datetime ( df1 [ 'dep_time' ], format = '%H:%M' ) a = df . assign ( dept_session = pd . cut ( df [ 'departure_t' ] . dt . hour , [ 0 , 6 , 12 , 18 , 24 ], labels = [ 'Night' , 'Morning' , 'Afternoon' , 'Evening' ])) df [ 'departure_S' ] = a [ 'dept_session' ] df Binning Arrival Time: arr_time = df [ 'arrival_time' ] . str . split ( ' ' ) . str [ 0 ] df [ 'arrival_t' ] = pd . to_datetime ( arr_time , format = '%H:%M' ) a = df . assign ( arr_session = pd . cut ( df [ 'arrival_t' ] . dt . hour , [ 0 , 6 , 12 , 18 , 24 ], labels = [ 'Night' , 'Morning' , 'Afternoon' , 'Evening' ])) df [ 'arrival_S' ] = a [ 'arr_session' ] df Visualizing what we just did: Within a few minutes of working with our data, we can hypothesize that a majority of people leave in the morning and majority of people arrive in the evening. This quick analysis can help airports focus customer service efforts based on frequencies of flyers to ride-share organizations optimizing drivers on hand for those rushing in and out of the airport. This is just one simple example of feature engineering - there's so much more to explore! Feature engineering is an important art of transforming raw data into features that better represent the data for machine learning and can make or break your model.","tags":"blog","url":"https://anitatea.github.io/blog/2019/12/feature-engineering.html","loc":"https://anitatea.github.io/blog/2019/12/feature-engineering.html"}]};